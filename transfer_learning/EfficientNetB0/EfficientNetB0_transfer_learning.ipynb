{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8450671,"sourceType":"datasetVersion","datasetId":5036109}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tempfile import TemporaryDirectory\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n\nimport torchvision\nfrom torchvision import datasets, models, transforms\n\nimport wandb\n\ncudnn.benchmark = True\nplt.ion()\n\n!wandb login d77624ba279c6354e2d27130c47fa3faf424ea9d\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-20T00:56:16.484796Z","iopub.execute_input":"2024-05-20T00:56:16.485671Z","iopub.status.idle":"2024-05-20T00:56:26.378183Z","shell.execute_reply.started":"2024-05-20T00:56:16.485608Z","shell.execute_reply":"2024-05-20T00:56:26.376918Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"# Customed Dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.classes = self._find_classes()\n        self.image_paths, self.labels = self._load_data()\n\n    def _find_classes(self):\n        classes = sorted([d for d in os.listdir(self.data_dir) if os.path.isdir(os.path.join(self.data_dir, d))])\n        return classes\n\n    def _load_data(self):\n        image_paths = []\n        labels = []\n        for label in self.classes:\n            class_dir = os.path.join(self.data_dir, label)\n            for img_name in os.listdir(class_dir):\n                img_path = os.path.join(class_dir, img_name)\n                image_paths.append(img_path)\n                labels.append(int(label))\n        return image_paths, labels\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n        label = int(label)\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-05-20T00:56:26.424356Z","iopub.execute_input":"2024-05-20T00:56:26.424790Z","iopub.status.idle":"2024-05-20T00:56:26.435679Z","shell.execute_reply.started":"2024-05-20T00:56:26.424758Z","shell.execute_reply":"2024-05-20T00:56:26.434610Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Parameters\n\n# Change batchsize to fit hardware\nbatch_size = 128\n\n# Training parameters\nnum_classes = 2139  \nlearning_rate = 0.001\nnum_epochs = 50\n\n# Scheduler\nstep_size = 7\ngamma = 0.1","metadata":{"execution":{"iopub.status.busy":"2024-05-20T00:56:58.718527Z","iopub.execute_input":"2024-05-20T00:56:58.718919Z","iopub.status.idle":"2024-05-20T00:56:58.723788Z","shell.execute_reply.started":"2024-05-20T00:56:58.718888Z","shell.execute_reply":"2024-05-20T00:56:58.722901Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Location of data\ntrain_dir = '/kaggle/input/wb-recognition-dataset2/wb_recognition_dataset/train'\nval_dir = '/kaggle/input/wb-recognition-dataset2/wb_recognition_dataset/val'","metadata":{"execution":{"iopub.status.busy":"2024-05-20T00:57:01.618801Z","iopub.execute_input":"2024-05-20T00:57:01.619476Z","iopub.status.idle":"2024-05-20T00:57:01.623705Z","shell.execute_reply.started":"2024-05-20T00:57:01.619443Z","shell.execute_reply":"2024-05-20T00:57:01.622693Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Data transformations\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),  \n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-20T00:57:04.240583Z","iopub.execute_input":"2024-05-20T00:57:04.240963Z","iopub.status.idle":"2024-05-20T00:57:04.247324Z","shell.execute_reply.started":"2024-05-20T00:57:04.240936Z","shell.execute_reply":"2024-05-20T00:57:04.246417Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Datasets from each folder\nimage_datasets = {\n    'train': CustomDataset(train_dir, data_transforms['train']),\n    'val': CustomDataset(val_dir, data_transforms['val']),\n}\n\n# Dataloader iterators\ndataloaders = {\n    'train': DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=4, pin_memory = True),\n    'val': DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=False, num_workers=4, pin_memory = True),\n}\n\n# Size of datasets\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}","metadata":{"execution":{"iopub.status.busy":"2024-05-20T01:01:44.716032Z","iopub.execute_input":"2024-05-20T01:01:44.716727Z","iopub.status.idle":"2024-05-20T01:02:20.744191Z","shell.execute_reply.started":"2024-05-20T01:01:44.716697Z","shell.execute_reply":"2024-05-20T01:02:20.743335Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Number of images and labels\nprint('Number of images in train: ', dataset_sizes['train'])\nprint('Number of labels in train: ',len(image_datasets['train'].classes))\nprint('Number of images in val: ', dataset_sizes['val'])\nprint('Number of labels in val: ', len(image_datasets['val'].classes))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T01:02:20.772809Z","iopub.execute_input":"2024-05-20T01:02:20.773061Z","iopub.status.idle":"2024-05-20T01:02:20.782036Z","shell.execute_reply.started":"2024-05-20T01:02:20.773038Z","shell.execute_reply":"2024-05-20T01:02:20.781090Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Number of images in train:  59009\nNumber of labels in train:  2130\nNumber of images in val:  1392\nNumber of labels in val:  595\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install efficientnet-pytorch","metadata":{"execution":{"iopub.status.busy":"2024-05-20T01:02:25.239211Z","iopub.execute_input":"2024-05-20T01:02:25.239874Z","iopub.status.idle":"2024-05-20T01:02:40.607337Z","shell.execute_reply.started":"2024-05-20T01:02:25.239839Z","shell.execute_reply":"2024-05-20T01:02:40.606023Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Collecting efficientnet-pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\nBuilding wheels for collected packages: efficientnet-pytorch\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=cdbd7584c705c7701481960f9c1b919e87bbe59434afabfae4baaa15981e7e8c\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\nSuccessfully built efficientnet-pytorch\nInstalling collected packages: efficientnet-pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from efficientnet_pytorch import EfficientNet\n# Load pretrained model\nmodel = EfficientNet.from_pretrained('efficientnet-b0')\n\n# Replace fully connected layer\nnum_ftrs = model._fc.in_features\nmodel._fc = nn.Linear(num_ftrs, num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T01:03:35.441413Z","iopub.execute_input":"2024-05-20T01:03:35.442040Z","iopub.status.idle":"2024-05-20T01:03:36.754891Z","shell.execute_reply.started":"2024-05-20T01:03:35.442008Z","shell.execute_reply":"2024-05-20T01:03:36.753928Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n100%|██████████| 20.4M/20.4M [00:00<00:00, 48.4MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b0\n","output_type":"stream"}]},{"cell_type":"code","source":"#adam\n# Training loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Decay the learning rate by 10% every 7 epochs\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size= step_size, gamma=gamma)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T01:03:39.565536Z","iopub.execute_input":"2024-05-20T01:03:39.566193Z","iopub.status.idle":"2024-05-20T01:03:39.572560Z","shell.execute_reply.started":"2024-05-20T01:03:39.566154Z","shell.execute_reply":"2024-05-20T01:03:39.571687Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Move to gpu \ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T01:03:42.660567Z","iopub.execute_input":"2024-05-20T01:03:42.660954Z","iopub.status.idle":"2024-05-20T01:03:42.850312Z","shell.execute_reply.started":"2024-05-20T01:03:42.660923Z","shell.execute_reply":"2024-05-20T01:03:42.849373Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check model\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T01:03:45.433488Z","iopub.execute_input":"2024-05-20T01:03:45.433868Z","iopub.status.idle":"2024-05-20T01:03:45.441796Z","shell.execute_reply.started":"2024-05-20T01:03:45.433837Z","shell.execute_reply":"2024-05-20T01:03:45.440671Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"EfficientNet(\n  (_conv_stem): Conv2dStaticSamePadding(\n    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n    (static_padding): ZeroPad2d((0, 1, 0, 1))\n  )\n  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n  (_blocks): ModuleList(\n    (0): MBConvBlock(\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n        (static_padding): ZeroPad2d((1, 1, 1, 1))\n      )\n      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        32, 8, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        8, 32, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (1): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n        (static_padding): ZeroPad2d((0, 1, 0, 1))\n      )\n      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        96, 4, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        4, 96, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (2): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n        (static_padding): ZeroPad2d((1, 1, 1, 1))\n      )\n      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        144, 6, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        6, 144, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (3): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n        (static_padding): ZeroPad2d((1, 2, 1, 2))\n      )\n      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        144, 6, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        6, 144, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (4): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n        (static_padding): ZeroPad2d((2, 2, 2, 2))\n      )\n      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        240, 10, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        10, 240, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (5): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n        (static_padding): ZeroPad2d((0, 1, 0, 1))\n      )\n      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        240, 10, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        10, 240, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (6-7): 2 x MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n        (static_padding): ZeroPad2d((1, 1, 1, 1))\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (8): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n        (static_padding): ZeroPad2d((2, 2, 2, 2))\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (9-10): 2 x MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n        (static_padding): ZeroPad2d((2, 2, 2, 2))\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (11): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n        (static_padding): ZeroPad2d((1, 2, 1, 2))\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (12-14): 3 x MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n        (static_padding): ZeroPad2d((2, 2, 2, 2))\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (15): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n        (static_padding): ZeroPad2d((1, 1, 1, 1))\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n  )\n  (_conv_head): Conv2dStaticSamePadding(\n    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n    (static_padding): Identity()\n  )\n  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n  (_dropout): Dropout(p=0.2, inplace=False)\n  (_fc): Linear(in_features=1280, out_features=2139, bias=True)\n  (_swish): MemoryEfficientSwish()\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, dataloaders, model_last_pth, model_best_pth, num_epochs):\n    \"\"\"Train a PyTorch Model\n\n    Params\n    --------\n        model (PyTorch model): cnn to train\n        criterion (PyTorch loss): objective to minimize\n        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n        dataloaders (PyTorch dataloader): dataloaders to iterate through\n        model_last_pth, model_best_pth (str ending in '.pt'): file path to save the model state dict\n        num_epochs (int): maximum number of training epochs\n\n    Returns\n    --------\n        model (PyTorch model): trained cnn with best weights\n    \"\"\"\n    \n    # Min validation loss\n    valid_loss_min = np.Inf\n    \n    # Main loop\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1} / {num_epochs}')\n        print('-' * 10)\n        \n        # Go through training and validation phase each epoch\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n            \n            # Keep track of loss and corrects each epoch\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Training loop\n            for inputs, labels in dataloaders[phase]:\n                inputs, labels = inputs.to(device), labels.to(device)\n                \n                # Clear gradients\n                optimizer.zero_grad()\n                \n                # Predicted outputs and loss of gradients\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # Backpropagation and update parameters\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # Update loss and number of correct predictions\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            \n            # Step the scheduler if in training phase\n            if phase == 'train':\n                scheduler.step()\n            \n            # Calculate loss and accuracy of each epoch\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            \n            \n            if phase == 'val':\n                if epoch_loss < valid_loss_min:\n                    valid_loss_min = epoch_loss\n                    torch.save(model.state_dict(), model_best_pth)\n            \n            print(f'{phase}\\t Loss: {epoch_loss:.4f}\\t Accuracy: {epoch_acc:.4f}')\n            \n            wandb.log({f'{phase}_loss': epoch_loss, f'{phase}_acc': epoch_acc})\n        \n        # Save model every epoch\n        torch.save(model.state_dict(), model_last_pth)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-20T01:03:52.153810Z","iopub.execute_input":"2024-05-20T01:03:52.154481Z","iopub.status.idle":"2024-05-20T01:03:52.167539Z","shell.execute_reply.started":"2024-05-20T01:03:52.154451Z","shell.execute_reply":"2024-05-20T01:03:52.166239Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Path to save model (last and best)\nmodel_last_pth = '/kaggle/working/efficientNetb0-imagenet-01-last.pt'\nmodel_best_pth = '/kaggle/working/efficientNetb0-imagenet-01-best.pt'\n\n# Saved model \nsaved_model_path = '/kaggle/working/efficientNetb0-imagenet-01-best.pt'","metadata":{"execution":{"iopub.status.busy":"2024-05-20T01:03:56.897176Z","iopub.execute_input":"2024-05-20T01:03:56.897534Z","iopub.status.idle":"2024-05-20T01:03:56.902103Z","shell.execute_reply.started":"2024-05-20T01:03:56.897505Z","shell.execute_reply":"2024-05-20T01:03:56.901158Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"wandb.init(project='ImageProcessing-project', sync_tensorboard=True)\n\n# Load saved model\ntry:\n    model.load_state_dict(torch.load(saved_model_path))\n    print('Loaded saved model successfully')\nexcept FileNotFoundError:\n    print('File not found')\nexcept Exception as e:\n    print(f'An error occurred: {e}')\n\nprint()\n\nmodel.to(device)\n    \nmodel = train_model(model, criterion, optimizer, dataloaders, model_last_pth, model_best_pth, num_epochs)\n\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T02:36:10.675609Z","iopub.execute_input":"2024-05-20T02:36:10.676396Z","iopub.status.idle":"2024-05-20T05:53:41.061526Z","shell.execute_reply.started":"2024-05-20T02:36:10.676359Z","shell.execute_reply":"2024-05-20T05:53:41.060691Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:f36qj97b) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▇▇▇██████████████████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▆▆▇▇▇▁▇███████████████</td></tr><tr><td>val_loss</td><td>▂▂▂▁▂█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.99778</td></tr><tr><td>train_loss</td><td>0.01021</td></tr><tr><td>val_acc</td><td>0.95187</td></tr><tr><td>val_loss</td><td>0.24324</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">colorful-field-96</strong> at: <a href='https://wandb.ai/ltl2702/ImageProcessing-project/runs/f36qj97b' target=\"_blank\">https://wandb.ai/ltl2702/ImageProcessing-project/runs/f36qj97b</a><br/> View project at: <a href='https://wandb.ai/ltl2702/ImageProcessing-project' target=\"_blank\">https://wandb.ai/ltl2702/ImageProcessing-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240520_010428-f36qj97b/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:f36qj97b). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240520_023610-sulcn47x</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ltl2702/ImageProcessing-project/runs/sulcn47x' target=\"_blank\">fresh-microwave-97</a></strong> to <a href='https://wandb.ai/ltl2702/ImageProcessing-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ltl2702/ImageProcessing-project' target=\"_blank\">https://wandb.ai/ltl2702/ImageProcessing-project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ltl2702/ImageProcessing-project/runs/sulcn47x' target=\"_blank\">https://wandb.ai/ltl2702/ImageProcessing-project/runs/sulcn47x</a>"},"metadata":{}},{"name":"stdout","text":"Loaded saved model successfully\n\nEpoch 1 / 50\n----------\ntrain\t Loss: 0.0134\t Accuracy: 0.9971\nval\t Loss: 0.2342\t Accuracy: 0.9497\nEpoch 2 / 50\n----------\ntrain\t Loss: 0.0133\t Accuracy: 0.9972\nval\t Loss: 0.2345\t Accuracy: 0.9511\nEpoch 3 / 50\n----------\ntrain\t Loss: 0.0129\t Accuracy: 0.9974\nval\t Loss: 0.2342\t Accuracy: 0.9511\nEpoch 4 / 50\n----------\ntrain\t Loss: 0.0128\t Accuracy: 0.9973\nval\t Loss: 0.2319\t Accuracy: 0.9511\nEpoch 5 / 50\n----------\ntrain\t Loss: 0.0133\t Accuracy: 0.9971\nval\t Loss: 0.2339\t Accuracy: 0.9526\nEpoch 6 / 50\n----------\ntrain\t Loss: 0.0129\t Accuracy: 0.9972\nval\t Loss: 0.2332\t Accuracy: 0.9533\nEpoch 7 / 50\n----------\ntrain\t Loss: 0.0125\t Accuracy: 0.9974\nval\t Loss: 0.2339\t Accuracy: 0.9519\nEpoch 8 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9976\nval\t Loss: 0.2341\t Accuracy: 0.9526\nEpoch 9 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9974\nval\t Loss: 0.2345\t Accuracy: 0.9526\nEpoch 10 / 50\n----------\ntrain\t Loss: 0.0127\t Accuracy: 0.9973\nval\t Loss: 0.2349\t Accuracy: 0.9533\nEpoch 11 / 50\n----------\ntrain\t Loss: 0.0125\t Accuracy: 0.9974\nval\t Loss: 0.2341\t Accuracy: 0.9526\nEpoch 12 / 50\n----------\ntrain\t Loss: 0.0129\t Accuracy: 0.9974\nval\t Loss: 0.2345\t Accuracy: 0.9519\nEpoch 13 / 50\n----------\ntrain\t Loss: 0.0127\t Accuracy: 0.9974\nval\t Loss: 0.2344\t Accuracy: 0.9526\nEpoch 14 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9973\nval\t Loss: 0.2343\t Accuracy: 0.9526\nEpoch 15 / 50\n----------\ntrain\t Loss: 0.0125\t Accuracy: 0.9974\nval\t Loss: 0.2340\t Accuracy: 0.9504\nEpoch 16 / 50\n----------\ntrain\t Loss: 0.0127\t Accuracy: 0.9975\nval\t Loss: 0.2329\t Accuracy: 0.9519\nEpoch 17 / 50\n----------\ntrain\t Loss: 0.0125\t Accuracy: 0.9974\nval\t Loss: 0.2339\t Accuracy: 0.9519\nEpoch 18 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9974\nval\t Loss: 0.2329\t Accuracy: 0.9511\nEpoch 19 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9975\nval\t Loss: 0.2337\t Accuracy: 0.9519\nEpoch 20 / 50\n----------\ntrain\t Loss: 0.0125\t Accuracy: 0.9975\nval\t Loss: 0.2342\t Accuracy: 0.9519\nEpoch 21 / 50\n----------\ntrain\t Loss: 0.0128\t Accuracy: 0.9973\nval\t Loss: 0.2334\t Accuracy: 0.9519\nEpoch 22 / 50\n----------\ntrain\t Loss: 0.0129\t Accuracy: 0.9973\nval\t Loss: 0.2345\t Accuracy: 0.9519\nEpoch 23 / 50\n----------\ntrain\t Loss: 0.0124\t Accuracy: 0.9975\nval\t Loss: 0.2335\t Accuracy: 0.9504\nEpoch 24 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9975\nval\t Loss: 0.2342\t Accuracy: 0.9519\nEpoch 25 / 50\n----------\ntrain\t Loss: 0.0127\t Accuracy: 0.9973\nval\t Loss: 0.2342\t Accuracy: 0.9526\nEpoch 26 / 50\n----------\ntrain\t Loss: 0.0127\t Accuracy: 0.9974\nval\t Loss: 0.2341\t Accuracy: 0.9526\nEpoch 27 / 50\n----------\ntrain\t Loss: 0.0123\t Accuracy: 0.9976\nval\t Loss: 0.2332\t Accuracy: 0.9519\nEpoch 28 / 50\n----------\ntrain\t Loss: 0.0127\t Accuracy: 0.9975\nval\t Loss: 0.2347\t Accuracy: 0.9526\nEpoch 29 / 50\n----------\ntrain\t Loss: 0.0127\t Accuracy: 0.9974\nval\t Loss: 0.2346\t Accuracy: 0.9519\nEpoch 30 / 50\n----------\ntrain\t Loss: 0.0129\t Accuracy: 0.9972\nval\t Loss: 0.2343\t Accuracy: 0.9511\nEpoch 31 / 50\n----------\ntrain\t Loss: 0.0127\t Accuracy: 0.9972\nval\t Loss: 0.2331\t Accuracy: 0.9519\nEpoch 32 / 50\n----------\ntrain\t Loss: 0.0125\t Accuracy: 0.9975\nval\t Loss: 0.2348\t Accuracy: 0.9519\nEpoch 33 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9973\nval\t Loss: 0.2341\t Accuracy: 0.9511\nEpoch 34 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9975\nval\t Loss: 0.2334\t Accuracy: 0.9511\nEpoch 35 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9975\nval\t Loss: 0.2334\t Accuracy: 0.9519\nEpoch 36 / 50\n----------\ntrain\t Loss: 0.0128\t Accuracy: 0.9973\nval\t Loss: 0.2335\t Accuracy: 0.9526\nEpoch 37 / 50\n----------\ntrain\t Loss: 0.0127\t Accuracy: 0.9974\nval\t Loss: 0.2354\t Accuracy: 0.9519\nEpoch 38 / 50\n----------\ntrain\t Loss: 0.0124\t Accuracy: 0.9975\nval\t Loss: 0.2340\t Accuracy: 0.9519\nEpoch 39 / 50\n----------\ntrain\t Loss: 0.0124\t Accuracy: 0.9974\nval\t Loss: 0.2341\t Accuracy: 0.9526\nEpoch 40 / 50\n----------\ntrain\t Loss: 0.0125\t Accuracy: 0.9973\nval\t Loss: 0.2337\t Accuracy: 0.9519\nEpoch 41 / 50\n----------\ntrain\t Loss: 0.0129\t Accuracy: 0.9974\nval\t Loss: 0.2335\t Accuracy: 0.9519\nEpoch 42 / 50\n----------\ntrain\t Loss: 0.0125\t Accuracy: 0.9975\nval\t Loss: 0.2344\t Accuracy: 0.9519\nEpoch 43 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9975\nval\t Loss: 0.2350\t Accuracy: 0.9519\nEpoch 44 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9974\nval\t Loss: 0.2341\t Accuracy: 0.9526\nEpoch 45 / 50\n----------\ntrain\t Loss: 0.0128\t Accuracy: 0.9973\nval\t Loss: 0.2355\t Accuracy: 0.9519\nEpoch 46 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9974\nval\t Loss: 0.2334\t Accuracy: 0.9526\nEpoch 47 / 50\n----------\ntrain\t Loss: 0.0128\t Accuracy: 0.9974\nval\t Loss: 0.2333\t Accuracy: 0.9511\nEpoch 48 / 50\n----------\ntrain\t Loss: 0.0128\t Accuracy: 0.9974\nval\t Loss: 0.2339\t Accuracy: 0.9519\nEpoch 49 / 50\n----------\ntrain\t Loss: 0.0126\t Accuracy: 0.9975\nval\t Loss: 0.2334\t Accuracy: 0.9511\nEpoch 50 / 50\n----------\ntrain\t Loss: 0.0125\t Accuracy: 0.9974\nval\t Loss: 0.2338\t Accuracy: 0.9526\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.017 MB uploaded\\r'), FloatProgress(value=0.07764548046285427, max=1.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▂▆▄▃▆█▆▆▅▆▄▆▆▅▆▄▄▇▆▅█▇▅▃▆▄▇▄▆▇▆▅▇▇▅▆▅▆▅</td></tr><tr><td>train_loss</td><td>██▅▄▅▃▃▃▂▅▄▃▄▂▃▃▄▅▂▃▄▁▄▄▃▃▃▃▅▃▂▂▅▃▃▃▃▄▄▃</td></tr><tr><td>val_acc</td><td>▁▄▄▄█▅▇▇▇▅▇▇▅▅▄▅▅▅▂▅▇▅▇▅▅▅▄▄▇▅▅▇▅▅▅▇▇▄▅▇</td></tr><tr><td>val_loss</td><td>▆▆▆▁▄▅▅▆▅▆▆▆▃▅▃▅▄▆▄▆▅▄▆▆▃▇▅▄▄█▅▅▄▆▇▅▄▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.99736</td></tr><tr><td>train_loss</td><td>0.01254</td></tr><tr><td>val_acc</td><td>0.95259</td></tr><tr><td>val_loss</td><td>0.23383</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fresh-microwave-97</strong> at: <a href='https://wandb.ai/ltl2702/ImageProcessing-project/runs/sulcn47x' target=\"_blank\">https://wandb.ai/ltl2702/ImageProcessing-project/runs/sulcn47x</a><br/> View project at: <a href='https://wandb.ai/ltl2702/ImageProcessing-project' target=\"_blank\">https://wandb.ai/ltl2702/ImageProcessing-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240520_023610-sulcn47x/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"#sgd\n# Training loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)\n\n# Decay the learning rate by 10% every 7 epochs\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size= step_size, gamma=gamma)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T02:35:30.740663Z","iopub.execute_input":"2024-05-20T02:35:30.741693Z","iopub.status.idle":"2024-05-20T02:35:30.751328Z","shell.execute_reply.started":"2024-05-20T02:35:30.741634Z","shell.execute_reply":"2024-05-20T02:35:30.750140Z"},"trusted":true},"execution_count":23,"outputs":[]}]}